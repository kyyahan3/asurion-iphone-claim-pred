{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:48:43.234820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import openpyxl\n",
    "import sklearn\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('apple_full_data_clean.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple iphone 11', 'apple iphone 12', 'apple iphone 13',\n",
       "       'apple iphone 14', 'apple iphone 3gs', 'apple iphone 4',\n",
       "       'apple iphone 4s', 'apple iphone 5', 'apple iphone 5s',\n",
       "       'apple iphone 6', 'apple iphone 6s', 'apple iphone 7',\n",
       "       'apple iphone 8', 'apple iphone se', 'apple iphone x',\n",
       "       'apple iphone xr', 'apple iphone xs'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('apple_full_data_clean.xlsx')['phone model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['phone color', \n",
    "    'week_of_month',\n",
    "    'month', 'phone size', 'phone model', 'year', 'day'], axis = 1, inplace = True)\n",
    "\n",
    "# One hot encode generation due to iPhones such as XS and XR\n",
    "one_hot = pd.get_dummies(data['generation'])\n",
    "data = data.drop('generation', axis = 1)\n",
    "data = data.join(one_hot)\n",
    "\n",
    "data[[\n",
    "    'claim',\n",
    "    'weeks_since_release',\n",
    "    'is_holiday'\n",
    "]] = data[['claim','weeks_since_release', 'is_holiday']].apply(np.float32)\n",
    "\n",
    "test_time = [\n",
    "    '2023-02-27',\n",
    "    '2023-02-20',\n",
    "    '2023-02-13',\n",
    "    '2023-02-06',\n",
    "    '2023-01-30',\n",
    "    '2023-01-23',\n",
    "    '2023-01-16',\n",
    "    '2023-01-09',\n",
    "    '2023-01-02',\n",
    "    '2022-12-26',\n",
    "    '2022-12-19'\n",
    "]\n",
    "\n",
    "testing_data = data.loc[data['weeks_monday'].isin(test_time)]\n",
    "training_data = data.loc[~data['weeks_monday'].isin(test_time)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim                  float32\n",
      "weeks_since_release    float32\n",
      "is_holiday             float32\n",
      "11                       uint8\n",
      "12                       uint8\n",
      "13                       uint8\n",
      "14                       uint8\n",
      "3gs                      uint8\n",
      "4                        uint8\n",
      "4s                       uint8\n",
      "5                        uint8\n",
      "5s                       uint8\n",
      "6                        uint8\n",
      "6s                       uint8\n",
      "7                        uint8\n",
      "8                        uint8\n",
      "se                       uint8\n",
      "x                        uint8\n",
      "xr                       uint8\n",
      "xs                       uint8\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/xhd08p257qs0hgklv6c67x640000gn/T/ipykernel_82840/2658106679.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_data.drop('weeks_monday', axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "training_time = pd.to_datetime(training_data['weeks_monday'], format='%Y-%m-%d')\n",
    "training_time = np.array(training_time)\n",
    "\n",
    "training_data.drop('weeks_monday', axis = 1, inplace = True)\n",
    "\n",
    "print(training_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update testing data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim                  float32\n",
      "weeks_since_release    float32\n",
      "is_holiday             float32\n",
      "11                       uint8\n",
      "12                       uint8\n",
      "13                       uint8\n",
      "14                       uint8\n",
      "3gs                      uint8\n",
      "4                        uint8\n",
      "4s                       uint8\n",
      "5                        uint8\n",
      "5s                       uint8\n",
      "6                        uint8\n",
      "6s                       uint8\n",
      "7                        uint8\n",
      "8                        uint8\n",
      "se                       uint8\n",
      "x                        uint8\n",
      "xr                       uint8\n",
      "xs                       uint8\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qc/xhd08p257qs0hgklv6c67x640000gn/T/ipykernel_82840/197915255.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testing_data.drop('weeks_monday', axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "testing_time = pd.to_datetime(testing_data['weeks_monday'], format='%Y-%m-%d')\n",
    "testing_time = np.array(testing_time)\n",
    "\n",
    "testing_data.drop('weeks_monday', axis = 1, inplace = True)\n",
    "\n",
    "print(testing_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = training_data['claim']\n",
    "X_train = training_data.drop(['claim'], axis=1)\n",
    "\n",
    "y_test = testing_data['claim']\n",
    "X_test = testing_data.drop(['claim'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        111.0\n",
       "1        319.0\n",
       "2        320.0\n",
       "3        305.0\n",
       "4        269.0\n",
       "         ...  \n",
       "17446     26.0\n",
       "17447     27.0\n",
       "17448     17.0\n",
       "17449     21.0\n",
       "17450     18.0\n",
       "Name: claim, Length: 15873, dtype: float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   weeks_since_release  is_holiday  11  12  13  14  3gs  4  4s  5  5s  6  6s  \\\n",
       " 0            92.428574         1.0   1   0   0   0    0  0   0  0   0  0   0   \n",
       " 1            93.428574         1.0   1   0   0   0    0  0   0  0   0  0   0   \n",
       " 2            94.428574         1.0   1   0   0   0    0  0   0  0   0  0   0   \n",
       " 3            95.428574         1.0   1   0   0   0    0  0   0  0   0  0   0   \n",
       " 4            96.428574         1.0   1   0   0   0    0  0   0  0   0  0   0   \n",
       " \n",
       "    7  8  se  x  xr  xs  \n",
       " 0  0  0   0  0   0   0  \n",
       " 1  0  0   0  0   0   0  \n",
       " 2  0  0   0  0   0   0  \n",
       " 3  0  0   0  0   0   0  \n",
       " 4  0  0   0  0   0   0  ,\n",
       " 0    111.0\n",
       " 1    319.0\n",
       " 2    320.0\n",
       " 3    305.0\n",
       " 4    269.0\n",
       " Name: claim, dtype: float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(), y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylQoAXTaXscV"
   },
   "source": [
    "## Build the Model\n",
    "\n",
    "You've seen these layers before and here is how it's looks like when combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the input data and target variable\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.fit_transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9251061 ,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       [-0.9163754 ,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       [-0.90764475,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       ...,\n",
       "       [ 0.17496155, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ],\n",
       "       [ 0.18369225, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ],\n",
       "       [ 0.19242294, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y6kJd40-0Hj9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 15:49:18.234148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 17, 64)            256       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 17, 128)           98816     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238,977\n",
      "Trainable params: 238,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    activation=\"relu\",\n",
    "    input_shape=[19, 1]\n",
    "  ),\n",
    "  tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.LSTM(128),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    " # Print the model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9251061 ,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       [-0.9163754 ,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       [-0.90764475,  2.1960914 ,  2.1985261 , ..., -0.13277976,\n",
       "        -0.27353963, -0.26912367],\n",
       "       ...,\n",
       "       [ 0.17496155, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ],\n",
       "       [ 0.18369225, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ],\n",
       "       [ 0.19242294, -0.45535448, -0.45485017, ..., -0.13277976,\n",
       "        -0.27353963,  3.715764  ]], dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer='adam'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3446.9607\n",
      "Epoch 2/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3452.8486\n",
      "Epoch 3/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3453.6033\n",
      "Epoch 4/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3454.0679\n",
      "Epoch 5/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3453.3337\n",
      "Epoch 6/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3456.8262\n",
      "Epoch 7/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3446.0068\n",
      "Epoch 8/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3457.5269\n",
      "Epoch 9/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3452.7366\n",
      "Epoch 10/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3452.7263\n",
      "Epoch 11/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3442.8464\n",
      "Epoch 12/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3443.8945\n",
      "Epoch 13/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3446.0283\n",
      "Epoch 14/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3445.6770\n",
      "Epoch 15/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3435.7158\n",
      "Epoch 16/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3447.7949\n",
      "Epoch 17/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3447.7686\n",
      "Epoch 18/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3448.7698\n",
      "Epoch 19/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3442.5054\n",
      "Epoch 20/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3443.6545\n",
      "Epoch 21/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3441.8486\n",
      "Epoch 22/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3449.7363\n",
      "Epoch 23/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3447.8784\n",
      "Epoch 24/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3450.5579\n",
      "Epoch 25/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3442.1345\n",
      "Epoch 26/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3450.3071\n",
      "Epoch 27/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3448.2107\n",
      "Epoch 28/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3440.9128\n",
      "Epoch 29/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3452.7139\n",
      "Epoch 30/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3444.8386\n",
      "Epoch 31/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3441.6228\n",
      "Epoch 32/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3446.7109\n",
      "Epoch 33/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3447.8472\n",
      "Epoch 34/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3438.4919\n",
      "Epoch 35/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3442.7229\n",
      "Epoch 36/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3439.4900\n",
      "Epoch 37/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3453.1494\n",
      "Epoch 38/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3447.0925\n",
      "Epoch 39/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3445.2854\n",
      "Epoch 40/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3442.0825\n",
      "Epoch 41/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3440.9780\n",
      "Epoch 42/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3447.8174\n",
      "Epoch 43/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3441.3691\n",
      "Epoch 44/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3445.9421\n",
      "Epoch 45/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3438.7227\n",
      "Epoch 46/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3442.6763\n",
      "Epoch 47/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3450.6536\n",
      "Epoch 48/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3444.8828\n",
      "Epoch 49/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3445.2512\n",
      "Epoch 50/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3450.4836\n",
      "Epoch 51/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3452.9497\n",
      "Epoch 52/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3444.2944\n",
      "Epoch 53/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3447.6677\n",
      "Epoch 54/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3446.9155\n",
      "Epoch 55/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3437.2344\n",
      "Epoch 56/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3440.8479\n",
      "Epoch 57/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3454.5625\n",
      "Epoch 58/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3442.5378\n",
      "Epoch 59/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3445.9639\n",
      "Epoch 60/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3448.4812\n",
      "Epoch 61/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3450.4326\n",
      "Epoch 62/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3443.0166\n",
      "Epoch 63/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3440.7646\n",
      "Epoch 64/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3448.8984\n",
      "Epoch 65/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3446.9922\n",
      "Epoch 66/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3454.7061\n",
      "Epoch 67/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3443.6458\n",
      "Epoch 68/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3444.8154\n",
      "Epoch 69/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3443.6177\n",
      "Epoch 70/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3445.8223\n",
      "Epoch 71/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3445.4900\n",
      "Epoch 72/150\n",
      "794/794 [==============================] - 27s 34ms/step - loss: 3446.7197\n",
      "Epoch 73/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3449.7095\n",
      "Epoch 74/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3446.1440\n",
      "Epoch 75/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3439.5073\n",
      "Epoch 76/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3448.0237\n",
      "Epoch 77/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3449.7415\n",
      "Epoch 78/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3441.7437\n",
      "Epoch 79/150\n",
      "794/794 [==============================] - 16s 21ms/step - loss: 3445.9395\n",
      "Epoch 80/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3443.9060\n",
      "Epoch 81/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3440.5320\n",
      "Epoch 82/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3443.9375\n",
      "Epoch 83/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3440.0593\n",
      "Epoch 84/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3444.8916\n",
      "Epoch 85/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3448.9658\n",
      "Epoch 86/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3447.9626\n",
      "Epoch 87/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3447.8745\n",
      "Epoch 88/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3437.6333\n",
      "Epoch 89/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3442.5103\n",
      "Epoch 90/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3448.7214\n",
      "Epoch 91/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3439.3545\n",
      "Epoch 92/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3445.0610\n",
      "Epoch 93/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3437.4045\n",
      "Epoch 94/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3457.4878\n",
      "Epoch 95/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3444.9785\n",
      "Epoch 96/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3440.4404\n",
      "Epoch 97/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3445.1838\n",
      "Epoch 98/150\n",
      "794/794 [==============================] - 14s 17ms/step - loss: 3445.5342\n",
      "Epoch 99/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3442.8496\n",
      "Epoch 100/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3448.8032\n",
      "Epoch 101/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3447.2009\n",
      "Epoch 102/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3444.9956\n",
      "Epoch 103/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3634.0657\n",
      "Epoch 104/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3503.0708\n",
      "Epoch 105/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3489.4885\n",
      "Epoch 106/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3498.4019\n",
      "Epoch 107/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3506.6086\n",
      "Epoch 108/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3488.2268\n",
      "Epoch 109/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3489.0037\n",
      "Epoch 110/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3502.9521\n",
      "Epoch 111/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3496.8989\n",
      "Epoch 112/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3494.7112\n",
      "Epoch 113/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3495.7205\n",
      "Epoch 114/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3492.2349\n",
      "Epoch 115/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3492.3994\n",
      "Epoch 116/150\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 3508.1907\n",
      "Epoch 117/150\n",
      "794/794 [==============================] - 16s 21ms/step - loss: 3471.3164\n",
      "Epoch 118/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3494.4858\n",
      "Epoch 119/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3501.6040\n",
      "Epoch 120/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3481.3499\n",
      "Epoch 121/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3492.0784\n",
      "Epoch 122/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3489.4802\n",
      "Epoch 123/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3484.1729\n",
      "Epoch 124/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3495.3843\n",
      "Epoch 125/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3469.0981\n",
      "Epoch 126/150\n",
      "794/794 [==============================] - 15s 18ms/step - loss: 3480.4390\n",
      "Epoch 127/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3490.9417\n",
      "Epoch 128/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3473.7793\n",
      "Epoch 129/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3491.6614\n",
      "Epoch 130/150\n",
      "794/794 [==============================] - 14s 18ms/step - loss: 3476.8064\n",
      "Epoch 131/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3480.5681\n",
      "Epoch 132/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3460.1714\n",
      "Epoch 133/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3487.2947\n",
      "Epoch 134/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3470.1128\n",
      "Epoch 135/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3471.6174\n",
      "Epoch 136/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3477.9312\n",
      "Epoch 137/150\n",
      "794/794 [==============================] - 18s 22ms/step - loss: 3469.8274\n",
      "Epoch 138/150\n",
      "794/794 [==============================] - 17s 21ms/step - loss: 3464.2002\n",
      "Epoch 139/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3468.1550\n",
      "Epoch 140/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3486.3657\n",
      "Epoch 141/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3469.8235\n",
      "Epoch 142/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3496.8137\n",
      "Epoch 143/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3466.8066\n",
      "Epoch 144/150\n",
      "794/794 [==============================] - 19s 24ms/step - loss: 3462.9019\n",
      "Epoch 145/150\n",
      "794/794 [==============================] - 19s 24ms/step - loss: 3478.2507\n",
      "Epoch 146/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3485.8298\n",
      "Epoch 147/150\n",
      "794/794 [==============================] - 16s 20ms/step - loss: 3473.7932\n",
      "Epoch 148/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3467.3672\n",
      "Epoch 149/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3447.6011\n",
      "Epoch 150/150\n",
      "794/794 [==============================] - 15s 19ms/step - loss: 3457.7881\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs = 150, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[76.72621],\n",
       "       [76.72621],\n",
       "       [76.72621],\n",
       "       ...,\n",
       "       [45.61362],\n",
       "       [45.61362],\n",
       "       [45.61362]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmape(actual, forecast):\n",
    "    \"\"\"Calculate Weighted Mean Absolute Percentage Error (WMAPE) with equal weights\"\"\"\n",
    " \n",
    "    numerator = np.sum(np.abs(actual - forecast)) * 100\n",
    "    denominator = np.sum(np.abs(actual))\n",
    "\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([218., 252., 233., ...,  11.,  17.,  12.], dtype=float32),\n",
       " array([76.72621, 76.72621, 76.72621, ..., 45.61362, 45.61362, 45.61362],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test), y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.93512287684086"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmape(np.array(y_test), y_pred.flatten())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "653b2c8856861abf632f93edfefbc7975c25a92111fedfa31eee4fe9d20d5c4d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit ('neuralnet': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
